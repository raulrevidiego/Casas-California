# -*- coding: utf-8 -*-
"""casas california.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BNFYebMjO5Nhd8BKKRjvZAkBO9ie0UuE
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import sklearn as sk
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import random

np.random.seed(42)
random.seed(42)




data= pd.read_csv('https://raw.githubusercontent.com/Adrian-Cancino/Machine-Learning-ScikitLearn/refs/heads/main/Conjuntos%20de%20datos/housing.csv')
data_num = data.select_dtypes(include=['float64', 'int64'])
data.head()

data.info()

data['ocean_proximity'].value_counts()

data.hist(bins=50, figsize=(20,15))

data.plot(kind='scatter', x='longitude', y='latitude', alpha = 0.1)

data.plot(kind='scatter', x='longitude', y='latitude', alpha = 0.4, s=data['population']/100, label='population', figsize=(10,5), c='median_house_value', cmap=plt.get_cmap('jet'), colorbar=True)

plt.figure(figsize=(10,5))
sns.heatmap(data_num.corr(), annot=True)

matriz_correlacion = data_num.corr()
matriz_correlacion['median_house_value'].sort_values(ascending=False)

data['total_bedrooms'].fillna(data['total_bedrooms'].median(), inplace=True)
data.info()

#vamos a probar combinando datos
data_combinada = data_num
data_combinada['rooms_per_house']= data['total_rooms']/data['households'] #vamos a ver como afectan la cantidad de habitaciones por vivienda
data_combinada['bedrooms_per_room'] = data['total_bedrooms']/data['total_rooms'] #vamos a ver como afectan la cantidad de cuartos por vivienda
data_combinada['population_per_house'] =data['population']/data['households'] #vamos a ver como afectan la cantidad de habitantes por vivienda
#vamos a crear otra matriz con estos datos combinados
matriz_combinada = data_combinada.corr()
matriz_combinada['median_house_value'].sort_values(ascending=False)



data_ocean = data[['ocean_proximity']]
ordinal_enconder = sk.preprocessing.OrdinalEncoder()
data_ocean_encoded = ordinal_enconder.fit_transform(data_ocean)
np.random.choice(data_ocean_encoded.ravel(), size=10)

cat_encoder = sk.preprocessing.OneHotEncoder()
data_ocean_1hot = cat_encoder.fit_transform(data_ocean)
data_ocean_1hot.toarray()

cat_encoder.categories_

encoded_df = pd.DataFrame(data_ocean_1hot.toarray(), columns=cat_encoder.get_feature_names_out())
encoded_df.head()

#algoritmos de ml
y = data_combinada['median_house_value'].values.reshape(-1,1)
#para variables dependientes escogemos la q mas correlacion tenga
x = data_combinada[[
  'median_income',
  'rooms_per_house',
  'total_rooms',
  'housing_median_age',
  'households',
  'bedrooms_per_room',
  'population_per_house'
]]
#agregamos las nuevas columnas de datos categoricos
data1 = pd.concat([x, encoded_df], axis=1)
data1.columns

x= data1.values

#regresion lineal multiple
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)

lin_reg = LinearRegression()
lin_reg.fit(x_train, y_train)

y_pred =lin_reg.predict(x_test)
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print (r2)

from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
x= sc_x.fit_transform(x)
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)
lin_reg = LinearRegression()
lin_reg.fit(x_train, y_train)
y_pred =lin_reg.predict(x_test)
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print (r2)

"""no hubo mejora con el escalado de datos"""

columnas =[
  'median_income',
  'rooms_per_house',
  'total_rooms',
  'housing_median_age',
  'households',
  'bedrooms_per_room',
  'population_per_house',
  'latitude',
  'longitude'
]
col_modelo= []
y =data_combinada['median_house_value'].values.reshape(-1,1)
for col in columnas:
  col_modelo.append(col)
  data1=data_combinada[col_modelo]
  data1 = pd.concat([data1,encoded_df], axis=1)
  x =data1.values
  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)
  lin_reg = LinearRegression()
  lin_reg.fit(x_train, y_train)
  y_pred =lin_reg.predict(x_test)
  r2 = r2_score(y_test, y_pred)
  print("Columnas:", col_modelo, "Calificación:", r2)

from sklearn.tree import DecisionTreeRegressor
columnas =[
  'median_income',
  'rooms_per_house',
  'total_rooms',
  'housing_median_age',
  'households',
  'bedrooms_per_room',
  'population_per_house',
  'latitude',
  'longitude'
]
col_modelo= []
y =data_combinada['median_house_value'].values.reshape(-1,1)
for col in columnas:
  col_modelo.append(col)
  data1=data_combinada[col_modelo]
  data1 = pd.concat([data1,encoded_df], axis=1)
  x =data1.values
  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)
  tree_reg = DecisionTreeRegressor()
  tree_reg.fit(x_train, y_train)
  y_pred = tree_reg.predict(x_test)
  r2 = r2_score(y_test, y_pred)
  print("Columnas:", col_modelo, "Calificación:", r2)

from sklearn.ensemble import RandomForestRegressor
columnas =[
  'median_income',
  'rooms_per_house',
  'total_rooms',
  'housing_median_age',
  'households',
  'bedrooms_per_room',
  'population_per_house',
  'latitude',
  'longitude'
]
col_modelo= []
y =data_combinada['median_house_value'].values
for col in columnas:
  col_modelo.append(col)
  data1=data_combinada[col_modelo]
  data1 = pd.concat([data1,encoded_df], axis=1)
  x =data1.values
  x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)
  forest_reg = RandomForestRegressor()
  forest_reg.fit(x_train, y_train)
  y_pred = forest_reg.predict(x_test)
  r2 = r2_score(y_test, y_pred)
  print("Columnas:", col_modelo, "Calificación:", r2)

